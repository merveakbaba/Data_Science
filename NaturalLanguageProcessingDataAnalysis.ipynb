{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d935f496-5522-48ed-9984-762b5ed45717",
   "metadata": {},
   "source": [
    "# NLP-Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "eeb28012-11e6-4b87-82ca-634458d016ff",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\akbab\\.anaconda\\anacon\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\akbab\\.anaconda\\anacon\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\akbab\\.anaconda\\anacon\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\akbab\\.anaconda\\anacon\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\akbab\\.anaconda\\anacon\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\akbab\\.anaconda\\anacon\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1137aaf7-e072-46b0-9ee1-bb1d8e0bdfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\akbab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "969ef506-c78e-4eb7-8766-db8564b28eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization #her kelimeyi virgülle ayırıp liste haline getirmek demek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "19c9ffc7-4866-412e-b191-dcc53b0bf83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text='Yaylaya gitmişti yayla zamanı, Gülizaar döndü de Döndü dönmedi. I hope you find it interesting'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "459b2681-59fb-4c0c-b478-1d59845d184f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "27e37756-8b71-45b1-9173-9bf9d00416a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yaylaya', 'gitmişti', 'yayla', 'zamanı,', 'Gülizaar', 'döndü', 'de', 'Döndü', 'dönmedi.', 'I', 'hope', 'you', 'find', 'it', 'interesting']\n"
     ]
    }
   ],
   "source": [
    "print(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c0fafc25-681e-4398-8fa1-a240761216a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "31a1fbf2-9beb-471c-a419-1baa50b9dc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yaylaya gitmişti yayla zamanı, Gülizaar döndü de Döndü dönmedi', ' I hope you find it interesting']\n"
     ]
    }
   ],
   "source": [
    "print(text.split('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "80676675-f39d-4e78-9214-9d4ae8ea900e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"list =[1,2]\n",
    "dictionary{'key':'aciklamasi'}\n",
    "tuple() #bunun içine yazılanlar değiştirilemez\n",
    "set() #sadece unique değerleri alıyor\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9d50d698-c157-4b56-a3fe-bf9d839126a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yaylaya gitmişti yayla zamanı, gülizaar döndü de döndü dönmedi. i hope you find it interesting'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fb8bb168-b464-4215-9d2a-f4eaab5f7d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'YAYLAYA GITMIŞTI YAYLA ZAMANI, GÜLIZAAR DÖNDÜ DE DÖNDÜ DÖNMEDI. I HOPE YOU FIND IT INTERESTING'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0e189195-c403-413b-b710-cce252cd5962",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1=\"John did not go to Bank although the banker called him\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3beeac6e-7786-45e5-8058-e7fe9c810df6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John',\n",
       " 'did',\n",
       " 'not',\n",
       " 'go',\n",
       " 'to',\n",
       " 'Bank',\n",
       " 'although',\n",
       " 'the',\n",
       " 'banker',\n",
       " 'called',\n",
       " 'him']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b49a955a-4992-487b-8388-d71368a38bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John', 'did', 'not', 'go', 'to', 'Bank', 'although', 'the', 'banker', 'called', 'him']\n"
     ]
    }
   ],
   "source": [
    "print(ex1.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "544d9b07-03ab-466e-a737-e4ec994e2e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "56b194be-8601-4f0b-bcdf-4fdd77a84ff2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John',\n",
       " 'did',\n",
       " 'not',\n",
       " 'go',\n",
       " 'to',\n",
       " 'Bank',\n",
       " 'although',\n",
       " 'the',\n",
       " 'banker',\n",
       " 'called',\n",
       " 'him']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r'\\W+',ex1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b1788171-2cb2-4951-8ad1-022fbd668b14",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1154153750.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[81], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(re.split(r'\\W+',ex1)\u001b[0m\n\u001b[1;37m                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "print(re.split(r'\\W+',ex1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "92aeecf6-8dd2-474a-b86a-4eeb0c23067f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize, WordPunctTokenizer, WhitespaceTokenizer, TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "af993501-ac4c-42f5-93dd-797ec017472c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John', 'did', 'not', 'go', 'to', 'Bank', 'although', 'the', 'banker', 'called', 'him']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(ex1)) # Resource punkt_tab not found.\n",
    "  #Please use the NLTK Downloader to obtain the resource ->hata verdi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "993450ee-f059-40dc-a77e-7279762c4672",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\akbab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bc88229c-300a-4f63-8e10-2bbef8de3e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John', 'did', 'not', 'go', 'to', 'Bank', 'although', 'the', 'banker', 'called', 'him']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(ex1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0a02b502-c4fb-40e7-bad2-f15c36756cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yaylaya gitmişti yayla zamanı, Gülizaar döndü de Döndü dönmedi.',\n",
       " 'I hope you find it interesting']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105876ab-8749-46dc-8e81-812608469044",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop words- gereksiz kelimeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dd54638e-efcb-4662-9008-92f23b8ad9f8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\akbab/nltk_data'\n    - 'C:\\\\Users\\\\akbab\\\\.anaconda\\\\anacon\\\\nltk_data'\n    - 'C:\\\\Users\\\\akbab\\\\.anaconda\\\\anacon\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\akbab\\\\.anaconda\\\\anacon\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\akbab\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\.anaconda\\anacon\\Lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\.anaconda\\anacon\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\akbab/nltk_data'\n    - 'C:\\\\Users\\\\akbab\\\\.anaconda\\\\anacon\\\\nltk_data'\n    - 'C:\\\\Users\\\\akbab\\\\.anaconda\\\\anacon\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\akbab\\\\.anaconda\\\\anacon\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\akbab\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m nltk\u001b[38;5;241m.\u001b[39mcorpus\u001b[38;5;241m.\u001b[39mstopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\.anaconda\\anacon\\Lib\\site-packages\\nltk\\corpus\\util.py:120\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__load()\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32m~\\.anaconda\\anacon\\Lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32m~\\.anaconda\\anacon\\Lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\.anaconda\\anacon\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\akbab/nltk_data'\n    - 'C:\\\\Users\\\\akbab\\\\.anaconda\\\\anacon\\\\nltk_data'\n    - 'C:\\\\Users\\\\akbab\\\\.anaconda\\\\anacon\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\akbab\\\\.anaconda\\\\anacon\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\akbab\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "nltk.corpus.stopwords.words('english') #   Resource stopwords not found.\n",
    "  # Please use the NLTK Downloader to obtain the resource:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b0fcb5eb-3368-4ae7-87bf-5f697774679c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\akbab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b46a78f5-c847-4500-a487-a3985964aca0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8daf94f9-fc35-43a4-8eca-036ba52af31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cec2dbf0-3d56-46a9-869e-de98218ecc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeb6fb8-9281-4249-acee-4c550c9bf83f",
   "metadata": {},
   "source": [
    "ps.stem('happiness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0f951eb2-f5bb-4c8e-b9ea-82554d86125c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'play'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('played')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "40f7504f-6def-4be6-b7ed-fa92fc7c7329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7cf3eec8-432f-42d9-a984-b73d2a459182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happy'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LancasterStemmer().stem('Happiness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0b50b44f-b23a-48c8-8e43-504f7348bba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "40ca0308-8747-4c7a-9f37-eab0dad05bb8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'porter',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SnowballStemmer.languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9b9e86b6-5ed4-4aab-afa9-7572ff9dfb21",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting TurkishStemmer\n",
      "  Downloading TurkishStemmer-1.3-py3-none-any.whl.metadata (10 kB)\n",
      "Downloading TurkishStemmer-1.3-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: TurkishStemmer\n",
      "Successfully installed TurkishStemmer-1.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install TurkishStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "563ff2a4-cd12-4add-ad6d-9ae477c79459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TurkishStemmer import TurkishStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f7dfb5c4-e3ff-4665-81f8-ce3808495cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'geliyor'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TurkishStemmer().stem('geliyorum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "01b384e1-4279-460a-ab05-8be9b9b839e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "frstemmer=SnowballStemmer('french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "067f7948-973d-4def-ab9a-82b7ab526647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bonjeur'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frstemmer.stem('bonjeur')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "98414118-79c5-4c62-a1ec-daa253f27855",
   "metadata": {},
   "outputs": [],
   "source": [
    "spstemmer=SnowballStemmer('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d08fdb7e-3618-47a5-9b83-a5ecfbaa097c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'com'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spstemmer.stem('comiendo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f334065-10b3-452b-aa06-feb692de5ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS-Part of Speech-Ozne nesne yuklem verb object fiil fail meful "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ead8eb71-a230-44d2-a4f8-ed0635a9edb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading avegared_perceptron_tagger: Package\n",
      "[nltk_data]     'avegared_perceptron_tagger' not found in index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('avegared_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "dc0b794b-ffb5-483f-b2b3-337a0f62a41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d3efd7c4-9956-4aa6-bd8d-48bdbf9363d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading avegared_perceptron_tagger: Package\n",
      "[nltk_data]     'avegared_perceptron_tagger' not found in index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('avegared_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1b12da62-9159-4f9d-ae35-1ba71173e5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumle='welcom readers.I hope yıu find it interesting .Please call me please'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2927ddac-cb7a-4078-976e-d0d1df88ca6e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\akbab/nltk_data'\n    - 'C:\\\\Users\\\\akbab\\\\.anaconda\\\\anacon\\\\nltk_data'\n    - 'C:\\\\Users\\\\akbab\\\\.anaconda\\\\anacon\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\akbab\\\\.anaconda\\\\anacon\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\akbab\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[123], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m nltk\u001b[38;5;241m.\u001b[39mpos_tag(cumle)\n",
      "File \u001b[1;32m~\\.anaconda\\anacon\\Lib\\site-packages\\nltk\\tag\\__init__.py:168\u001b[0m, in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpos_tag\u001b[39m(tokens, tagset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    144\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;124;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;124;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;124;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m _get_tagger(lang)\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[1;32m~\\.anaconda\\anacon\\Lib\\site-packages\\nltk\\tag\\__init__.py:110\u001b[0m, in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    108\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m PerceptronTagger(lang\u001b[38;5;241m=\u001b[39mlang)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m PerceptronTagger()\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
      "File \u001b[1;32m~\\.anaconda\\anacon\\Lib\\site-packages\\nltk\\tag\\perceptron.py:183\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[1;34m(self, load, lang)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_from_json(lang)\n",
      "File \u001b[1;32m~\\.anaconda\\anacon\\Lib\\site-packages\\nltk\\tag\\perceptron.py:273\u001b[0m, in \u001b[0;36mPerceptronTagger.load_from_json\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;66;03m# Automatically find path to the tagger if location is not specified.\u001b[39;00m\n\u001b[1;32m--> 273\u001b[0m     loc \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtaggers/averaged_perceptron_tagger_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(loc \u001b[38;5;241m+\u001b[39m TAGGER_JSONS[lang][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(fin)\n",
      "File \u001b[1;32m~\\.anaconda\\anacon\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\akbab/nltk_data'\n    - 'C:\\\\Users\\\\akbab\\\\.anaconda\\\\anacon\\\\nltk_data'\n    - 'C:\\\\Users\\\\akbab\\\\.anaconda\\\\anacon\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\akbab\\\\.anaconda\\\\anacon\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\akbab\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "nltk.pos_tag(cumle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "10092037-2a28-49e6-85c1-9c1d118730d5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autocorrect\n",
      "  Downloading autocorrect-2.6.1.tar.gz (622 kB)\n",
      "     ---------------------------------------- 0.0/622.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/622.8 kB ? eta -:--:--\n",
      "     -------------------------------------- 622.8/622.8 kB 3.8 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: autocorrect\n",
      "  Building wheel for autocorrect (setup.py): started\n",
      "  Building wheel for autocorrect (setup.py): finished with status 'done'\n",
      "  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622375 sha256=32442e4e831f422896b81bdbbdc3ef8172fd98bb5748692506a03074f8f7c9fe\n",
      "  Stored in directory: c:\\users\\akbab\\appdata\\local\\pip\\cache\\wheels\\b6\\28\\c2\\9ddf8f57f871b55b6fd0ab99c887531fb9a66e5ff236b82aee\n",
      "Successfully built autocorrect\n",
      "Installing collected packages: autocorrect\n",
      "Successfully installed autocorrect-2.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "61b743cc-50aa-4586-b007-3c97bcc20318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import spell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9ee8f8bf-2cc8-4393-acac-02c4af3dc64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Held'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell('Helo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b4cf212b-9d90-46de-9371-ea037b23be08",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: nltk>=3.9 in c:\\users\\akbab\\.anaconda\\anacon\\lib\\site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\akbab\\.anaconda\\anacon\\lib\\site-packages (from nltk>=3.9->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\akbab\\.anaconda\\anacon\\lib\\site-packages (from nltk>=3.9->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\akbab\\.anaconda\\anacon\\lib\\site-packages (from nltk>=3.9->textblob) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\akbab\\.anaconda\\anacon\\lib\\site-packages (from nltk>=3.9->textblob) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\akbab\\.anaconda\\anacon\\lib\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n",
      "Downloading textblob-0.19.0-py3-none-any.whl (624 kB)\n",
      "   ---------------------------------------- 0.0/624.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/624.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 624.3/624.3 kB 4.6 MB/s eta 0:00:00\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.19.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3e93cc06-76ba-44be-ba4e-48a4b395d6e7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob==0.17.1\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\akbab\\.anaconda\\anacon\\lib\\site-packages (from textblob==0.17.1) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\akbab\\.anaconda\\anacon\\lib\\site-packages (from nltk>=3.1->textblob==0.17.1) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\akbab\\.anaconda\\anacon\\lib\\site-packages (from nltk>=3.1->textblob==0.17.1) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\akbab\\.anaconda\\anacon\\lib\\site-packages (from nltk>=3.1->textblob==0.17.1) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\akbab\\.anaconda\\anacon\\lib\\site-packages (from nltk>=3.1->textblob==0.17.1) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\akbab\\.anaconda\\anacon\\lib\\site-packages (from click->nltk>=3.1->textblob==0.17.1) (0.4.6)\n",
      "Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "   ---------------------------------------- 0.0/636.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/636.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 636.8/636.8 kB 4.8 MB/s eta 0:00:00\n",
      "Installing collected packages: textblob\n",
      "  Attempting uninstall: textblob\n",
      "    Found existing installation: textblob 0.19.0\n",
      "    Uninstalling textblob-0.19.0:\n",
      "      Successfully uninstalled textblob-0.19.0\n",
      "Successfully installed textblob-0.17.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob==0.17.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "61943b9e-cb31-46df-b78d-541268e16d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6d5aab9c-5f9d-44be-bdfd-627a931eac8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "b=TextBlob('I havee goad spelling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0079dc60-b68d-454d-bc18-1ce3205ac50d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"I have good spelling\")"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "607ca1e3-bd48-42f6-9b48-ad960b535ed6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     ------------------------------ ------- 786.4/981.5 kB 4.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- 981.5/981.5 kB 4.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six in c:\\users\\akbab\\.anaconda\\anacon\\lib\\site-packages (from langdetect) (1.16.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993251 sha256=f490c2c7556129df67e8f47fc4ec32895060c078fb9156293a8f96a58006d461\n",
      "  Stored in directory: c:\\users\\akbab\\appdata\\local\\pip\\cache\\wheels\\c1\\67\\88\\e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "53269cc3-1ccd-4c57-8921-7376131f6cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b837b479-c616-437d-81f6-fba61fd85fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neuralink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4862e738-0375-4644-9828-9580769adea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "d=detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "64d7298e-8956-4900-afd8-72c5310237fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tr'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d('Döndü dönmedi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "952dde1a-b297-4b14-bc72-aa370abc9887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bg'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d('До свидания')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "eef9c027-06e7-47cf-8ef5-ca145568d5ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ar'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d('أَعْمَلُ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b2654ecd-40fc-4a14-9efa-aa6e93a0ea23",
   "metadata": {},
   "outputs": [],
   "source": [
    "b=TextBlob('I am free blackman from Africa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "12d7e26f-204f-4bc5-b46d-b4381471d0cd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TextBlob' object has no attribute 'translate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[151], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m b\u001b[38;5;241m.\u001b[39mtranslate(from_lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mengish\u001b[39m\u001b[38;5;124m'\u001b[39m,to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TextBlob' object has no attribute 'translate'"
     ]
    }
   ],
   "source": [
    "b.translate(from_lang='engish',to='tr')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "80171f88-96e0-4d6e-aaf2-e9390da17575",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# bir nlp projeisnde yapmamız gerekn şeyler\n",
    "1- Datayı oku \n",
    "2- herseyi küçük harfe çevir . A=065 a=097\n",
    "3-noktalama işsretlerini kaldır\n",
    "4-rakamları kaldır\n",
    "5- satır sonlarını kaldır\n",
    "6-stop wordsleri kaldır(gereksiz kelimeler)\n",
    "7-tokenize et\n",
    "8- ekleri kaldırıp kökleri bul \n",
    "9- vectorize et\n",
    "10- TF/IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9c7c05c5-42e0-4fae-9e9c-9b7566b27a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=['call you tonight','call me a cab','please call me...PLEASE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "8705b987-8750-4f5f-987a-22c0e8c40e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "127c3f10-cde8-4233-bf41-a22efbb49165",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect=CountVectorizer()#vect=CountVectorizer(lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "8b4fdbfc-7fd1-407d-815b-c025dc9cd98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "855cfeda-6808-428a-917f-207c4455b5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf=pd.DataFrame(vect.fit_transform(text).toarray(),columns=vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b37f49ce-8974-404b-8d1a-64ecaf9d5938",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>ne</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  ne  please  tonight  you\n",
       "0    0     1   0   0       0        1    1\n",
       "1    1     1   0   1       0        0    0\n",
       "2    0     1   1   0       2        0    0"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8dfeb4d4-2e59-4952-9ec2-756d5780abcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=vect.fit_transform(text).toarray().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "59820193-856b-44cb-9bf5-f0ce92f90586",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>ne</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab      call   me   ne  please  tonight  you\n",
       "0  0.0  0.333333  0.0  0.0     0.0      1.0  1.0\n",
       "1  1.0  0.333333  0.0  1.0     0.0      0.0  0.0\n",
       "2  0.0  0.333333  1.0  0.0     1.0      0.0  0.0"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf/df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "e1052795-2856-49bb-9c04-5402644661c1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'read_cvs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[161], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_cvs(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myelp.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'read_cvs'"
     ]
    }
   ],
   "source": [
    "df=pd.read_cvs('yelp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "087e08f2-3e5b-4839-9df7-b7105f12976f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[162], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "8d848f11-880b-4be1-8fdb-8963d412978a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call you tonight\n",
      "call ne a cab\n",
      "please call me...please\n"
     ]
    }
   ],
   "source": [
    "for i in text:\n",
    "    print(i.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "cd74c2fe-1706-4626-8b34-15256aa53a49",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\akbab\\AppData\\Local\\Temp\\ipykernel_23860\\2683497073.py:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  df['text']=df['text'].str.replace('[^\\w\\s]','',regex=True)\n",
      "C:\\Users\\akbab\\AppData\\Local\\Temp\\ipykernel_23860\\2683497073.py:3: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  df['text']=df['text'].str.lower('\\d+','',regex=True)\n",
      "C:\\Users\\akbab\\AppData\\Local\\Temp\\ipykernel_23860\\2683497073.py:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  df['text']=df['text'].str.replace('[^\\w\\s]','',regex=True)\n",
      "C:\\Users\\akbab\\AppData\\Local\\Temp\\ipykernel_23860\\2683497073.py:3: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  df['text']=df['text'].str.lower('\\d+','',regex=True)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[169], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m      2\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlower(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "df['text']=df['text'].str.lower()\n",
    "df['text']=df['text'].str.replace('[^\\w\\s]','',regex=True)\n",
    "df['text']=df['text'].str.lower('\\d+','',regex=True)\n",
    "df['text']=df['text'].str.lower('\\n','',regex=True)\n",
    "df['text']=df['text'].str.lower('\\r','',regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ec139668-9b6a-4a7d-bc15-434053d20c31",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[165], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "96f72141-a353-4301-8006-0f875ae4c557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7,)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "62bc0267-d15e-4b5e-941b-9fe78b0bcbeb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[170], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c2e0fca-3319-4cc1-874d-aa30310452ab",
   "metadata": {},
   "source": [
    "Sentiment Analiysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "03632f4f-e3ae-4c25-89ef-1390a7a0aa7e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'stars'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[172], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ybw\u001b[38;5;241m=\u001b[39mdf[(df\u001b[38;5;241m.\u001b[39mstars\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m|\u001b[39m(df\u001b[38;5;241m.\u001b[39mstars\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m5\u001b[39m)]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'stars'"
     ]
    }
   ],
   "source": [
    "ybw=df[(df.stars==1)|(df.stars==5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "c2320888-2b89-49e3-bcc6-871113e4c867",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ybw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[173], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ybw\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ybw' is not defined"
     ]
    }
   ],
   "source": [
    "ybw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "b85e3aea-3429-40fd-bca5-622efc5211c5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ybw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[174], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ybw\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ybw' is not defined"
     ]
    }
   ],
   "source": [
    "ybw.reset_index(drop=True,inplace=True) # ybw=ybw.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "28e5c4d2-ec36-4e6a-bce9-e55aca17c640",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ybw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[175], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ybw\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ybw' is not defined"
     ]
    }
   ],
   "source": [
    "ybw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "48d2ef55-fd69-4d9f-9958-2b778b18e180",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[176], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x\u001b[38;5;241m=\u001b[39mdf[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m      2\u001b[0m y\u001b[38;5;241m=\u001b[39mdf[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstars\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "x=yb[['text']]\n",
    "y=df[['stars']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "6cfaa471-d65d-4835-94a2-f9c8167dbc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "a08d60de-8b50-41f0-ab13-bc07a7979cff",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[178], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "1568a850-17c9-48dc-9b0c-e2fecaab4474",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CountVectorizer.__init__() got an unexpected keyword argument 'stop_word'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[189], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m vect\u001b[38;5;241m=\u001b[39mCountVectorizer(stop_word\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: CountVectorizer.__init__() got an unexpected keyword argument 'stop_word'"
     ]
    }
   ],
   "source": [
    "vect=CountVectorizer(stop_word='english',ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "0f4e346a-8444-4767-9e10-eb35abbf9da8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[182], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m xyeni\u001b[38;5;241m=\u001b[39mvect\u001b[38;5;241m.\u001b[39mfit_transform(x[text])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "xyeni=vect.fit_transform(x['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "be35fa7e-aa1c-4387-b074-801c9119d93c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trsin_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[183], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x_train,x_test,y_train,y_test\u001b[38;5;241m=\u001b[39mtrsin_test_split(xyeni,y,random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.20\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trsin_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test=trsin_test_split(xyeni,y,random_state=42,test_size=.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "a4a4e121-b649-49bd-86e5-e1e01d8b43fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb=MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "2d55b16b-a0ac-4071-82cb-6cff36a34beb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[185], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[38;5;241m=\u001b[39mnb\u001b[38;5;241m.\u001b[39mfit(x_train,y_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "model=nb.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "f993fdbd-f01b-45cf-b824-1f86f7106091",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[186], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pred\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(x_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "pred=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "55b72abd-5c92-4a9e-b6e6-f3eb45160214",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.mmetrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[187], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.mmetrics'"
     ]
    }
   ],
   "source": [
    "from sklearn.mmetrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "18a097e5-8b63-42fb-9317-8c6f69288645",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracy_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[188], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m accuracy_score(y_tezt,pred)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'accuracy_score' is not defined"
     ]
    }
   ],
   "source": [
    "accuracy_score(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "859167ef-7f38-4608-a05b-d638c0e98808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP NaetText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "91ae6e35-90f3-4cf9-8333-7352262a981e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting neattextNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading neattext-0.1.3-py3-none-any.whl.metadata (12 kB)\n",
      "Downloading neattext-0.1.3-py3-none-any.whl (114 kB)\n",
      "Installing collected packages: neattext\n",
      "Successfully installed neattext-0.1.3\n"
     ]
    }
   ],
   "source": [
    "pip install neattext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "b14d1096-fcc9-4a82-bb48-a6e78948aa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neattext.functions as nfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "f71fd83b-3405-4095-b117-7e809d687972",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BTC_ADDRESS_REGEX',\n",
       " 'CURRENCY_REGEX',\n",
       " 'CURRENCY_SYMB_REGEX',\n",
       " 'Counter',\n",
       " 'DATE_REGEX',\n",
       " 'EMAIL_REGEX',\n",
       " 'EMOJI_REGEX',\n",
       " 'HASTAG_REGEX',\n",
       " 'MASTERCard_REGEX',\n",
       " 'MD5_SHA_REGEX',\n",
       " 'MOST_COMMON_PUNCT_REGEX',\n",
       " 'NUMBERS_REGEX',\n",
       " 'PHONE_REGEX',\n",
       " 'PoBOX_REGEX',\n",
       " 'SPECIAL_CHARACTERS_REGEX',\n",
       " 'STOPWORDS',\n",
       " 'STOPWORDS_de',\n",
       " 'STOPWORDS_en',\n",
       " 'STOPWORDS_es',\n",
       " 'STOPWORDS_fr',\n",
       " 'STOPWORDS_ru',\n",
       " 'STOPWORDS_yo',\n",
       " 'STREET_ADDRESS_REGEX',\n",
       " 'TextFrame',\n",
       " 'URL_PATTERN',\n",
       " 'USER_HANDLES_REGEX',\n",
       " 'VISACard_REGEX',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__generate_text',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__numbers_dict',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_lex_richness_herdan',\n",
       " '_lex_richness_maas_ttr',\n",
       " 'clean_text',\n",
       " 'defaultdict',\n",
       " 'digit2words',\n",
       " 'extract_btc_address',\n",
       " 'extract_currencies',\n",
       " 'extract_currency_symbols',\n",
       " 'extract_dates',\n",
       " 'extract_emails',\n",
       " 'extract_emojis',\n",
       " 'extract_hashtags',\n",
       " 'extract_html_tags',\n",
       " 'extract_mastercard_addr',\n",
       " 'extract_md5sha',\n",
       " 'extract_numbers',\n",
       " 'extract_pattern',\n",
       " 'extract_phone_numbers',\n",
       " 'extract_postoffice_box',\n",
       " 'extract_shortwords',\n",
       " 'extract_special_characters',\n",
       " 'extract_stopwords',\n",
       " 'extract_street_address',\n",
       " 'extract_terms_in_bracket',\n",
       " 'extract_urls',\n",
       " 'extract_userhandles',\n",
       " 'extract_visacard_addr',\n",
       " 'fix_contractions',\n",
       " 'generate_sentence',\n",
       " 'hamming_distance',\n",
       " 'inverse_df',\n",
       " 'lexical_richness',\n",
       " 'markov_chain',\n",
       " 'math',\n",
       " 'nlargest',\n",
       " 'normalize',\n",
       " 'num2words',\n",
       " 'random',\n",
       " 're',\n",
       " 'read_txt',\n",
       " 'remove_accents',\n",
       " 'remove_bad_quotes',\n",
       " 'remove_btc_address',\n",
       " 'remove_currencies',\n",
       " 'remove_currency_symbols',\n",
       " 'remove_custom_pattern',\n",
       " 'remove_custom_words',\n",
       " 'remove_dates',\n",
       " 'remove_emails',\n",
       " 'remove_emojis',\n",
       " 'remove_hashtags',\n",
       " 'remove_html_tags',\n",
       " 'remove_mastercard_addr',\n",
       " 'remove_md5sha',\n",
       " 'remove_multiple_spaces',\n",
       " 'remove_non_ascii',\n",
       " 'remove_numbers',\n",
       " 'remove_phone_numbers',\n",
       " 'remove_postoffice_box',\n",
       " 'remove_puncts',\n",
       " 'remove_punctuations',\n",
       " 'remove_shortwords',\n",
       " 'remove_special_characters',\n",
       " 'remove_stopwords',\n",
       " 'remove_street_address',\n",
       " 'remove_terms_in_bracket',\n",
       " 'remove_urls',\n",
       " 'remove_userhandles',\n",
       " 'remove_visacard_addr',\n",
       " 'replace_bad_quotes',\n",
       " 'replace_currencies',\n",
       " 'replace_currency_symbols',\n",
       " 'replace_dates',\n",
       " 'replace_emails',\n",
       " 'replace_emojis',\n",
       " 'replace_numbers',\n",
       " 'replace_phone_numbers',\n",
       " 'replace_special_characters',\n",
       " 'replace_term',\n",
       " 'replace_urls',\n",
       " 'string',\n",
       " 'term_freq',\n",
       " 'to_txt',\n",
       " 'unicodedata',\n",
       " 'word_freq',\n",
       " 'word_length_freq']"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(nfx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "0f2f4e10-c6b2-4ec8-b071-fa5447e280f1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[194], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "df['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "4c4cc2b4-95cb-421a-91d7-88f0ea867b49",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[195], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "''.join(df['text'].tolist()) # bütün elemanları birleştirme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "75c645a7-b78b-4f68-adf6-66eb316c7aac",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[196], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m s\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "s=''.join(df['text'].tolist()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "cafb9336-12bf-4cbb-b5db-59c349fe2efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "8ada0310-4852-4975-be6e-72ed78307492",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[198], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Counter(s\u001b[38;5;241m.\u001b[39msplit())\n",
      "\u001b[1;31mNameError\u001b[0m: name 's' is not defined"
     ]
    }
   ],
   "source": [
    "Counter(s.split())  # en çok tekrar eden kelime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "1413f69d-4de3-40ce-908c-d03ed51b6fba",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[199], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m s2\u001b[38;5;241m=\u001b[39mnfx\u001b[38;5;241m.\u001b[39mremove_stopwords(s)\n",
      "\u001b[1;31mNameError\u001b[0m: name 's' is not defined"
     ]
    }
   ],
   "source": [
    "s2=nfx.remove_stopwords(s) # bütün gereksiz kelimeleri kaldrıma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "7c754407-f85b-4775-9d3b-11ad8e436844",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[200], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Counter(s2\u001b[38;5;241m.\u001b[39msplit())\n",
      "\u001b[1;31mNameError\u001b[0m: name 's2' is not defined"
     ]
    }
   ],
   "source": [
    "Counter(s2.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "52ee57d5-a465-4f0d-8808-3efac17ffe81",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[201], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data\u001b[38;5;241m=\u001b[39mCounter(s2\u001b[38;5;241m.\u001b[39msplit())\u001b[38;5;241m.\u001b[39mmost_common(\u001b[38;5;241m30\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 's2' is not defined"
     ]
    }
   ],
   "source": [
    "data=Counter(s2.split()).most_common(30)  # en çok tekrar eden 30 kelime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "0e099253-39eb-46a5-9e9e-f9e78b44701d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[202], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "29e022b5-b35a-4603-b5dd-c484468a1391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "9226c3f4-ea1e-49a9-9e3d-59b3b2f60e8e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ')' (1041457472.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[209], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    plt.figure(figsize=12.8))\u001b[0m\n\u001b[1;37m                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unmatched ')'\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=12.8))\n",
    "plt.bar(data.keys(),data.values())\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "17ff2f8d-cc9b-4bde-aa6d-ad7dffc2dbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "d6ad5e05-0e72-4429-a762-21feec53cf3d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ')' (3365203175.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[210], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    plt.figure(figsize=12.8))\u001b[0m\n\u001b[1;37m                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unmatched ')'\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=12.8))\n",
    "sns.barplot(x=data.keys(),y=data.values(),hue=data.keys())\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "5724c330-cdac-4948-970e-df8c959d8e25",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[211], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x:nfx\u001b[38;5;241m.\u001b[39mclean_test(x))\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "df['text']=df['text'].apply(lambda x:nfx.clean_test(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "9442853f-6ee5-4b57-a773-e3e35c93d7bd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[212], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x:nfx\u001b[38;5;241m.\u001b[39mnormalize(x))\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "df['text']=df['text'].apply(lambda x:nfx.normalize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "0ca22fe1-b7d7-4753-936d-09e7bcc9666c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[213], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "df9383a1-1c36-4ee8-9533-3f621b1e52a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kokbul(text):\n",
    "    words=Textblob(text).words\n",
    "    return [word.lemmatize() for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb383bb-0d8f-40b2-9e9a-d749ac53a8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumle=\"Zafer loves NLP, working hard, making dedication\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "3b67328e-3ce5-406f-92db-63e6821703dc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The `text` argument passed to `__init__(text)` must be a string, not <class 'list'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[215], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m TextBlob(cumle)\u001b[38;5;241m.\u001b[39mwords\n",
      "File \u001b[1;32m~\\.anaconda\\anacon\\Lib\\site-packages\\textblob\\blob.py:371\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, text, tokenizer, pos_tagger, np_extractor, analyzer, parser, classifier, clean_html)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBaseBlob\u001b[39;00m(StringlikeMixin, BlobComparableMixin):\n\u001b[0;32m    353\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"An abstract base class that all textblob classes will inherit from.\u001b[39;00m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;124;03m    Includes words, POS tag, NP, and word count properties. Also includes\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m    basic dunder and string methods for making objects like Python strings.\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \n\u001b[0;32m    357\u001b[0m \u001b[38;5;124;03m    :param text: A string.\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;124;03m    :param tokenizer: (optional) A tokenizer instance. If ``None``,\u001b[39;00m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;124;03m        defaults to :class:`WordTokenizer() <textblob.tokenizers.WordTokenizer>`.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;124;03m    :param np_extractor: (optional) An NPExtractor instance. If ``None``,\u001b[39;00m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m        defaults to :class:`FastNPExtractor() <textblob.en.np_extractors.FastNPExtractor>`.\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;124;03m    :param pos_tagger: (optional) A Tagger instance. If ``None``,\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;124;03m        defaults to :class:`NLTKTagger <textblob.en.taggers.NLTKTagger>`.\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;124;03m    :param analyzer: (optional) A sentiment analyzer. If ``None``,\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;124;03m        defaults to :class:`PatternAnalyzer <textblob.en.sentiments.PatternAnalyzer>`.\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;124;03m    :param parser: A parser. If ``None``, defaults to\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;124;03m        :class:`PatternParser <textblob.en.parsers.PatternParser>`.\u001b[39;00m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;124;03m    :param classifier: A classifier.\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \n\u001b[0;32m    370\u001b[0m \u001b[38;5;124;03m    .. versionchanged:: 0.6.0\u001b[39;00m\n\u001b[1;32m--> 371\u001b[0m \u001b[38;5;124;03m        ``clean_html`` parameter deprecated, as it was in NLTK.\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    373\u001b[0m     np_extractor \u001b[38;5;241m=\u001b[39m FastNPExtractor()\n\u001b[0;32m    374\u001b[0m     pos_tagger \u001b[38;5;241m=\u001b[39m NLTKTagger()\n",
      "\u001b[1;31mTypeError\u001b[0m: The `text` argument passed to `__init__(text)` must be a string, not <class 'list'>"
     ]
    }
   ],
   "source": [
    "TextBlob(cumle).words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "427e4a03-d72f-4d8c-94a3-bfe6ac1e0f49",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The `text` argument passed to `__init__(text)` must be a string, not <class 'list'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[217], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m [word\u001b[38;5;241m.\u001b[39mlemmatize() \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m TextBlob(cumle)\u001b[38;5;241m.\u001b[39mwords]\n",
      "File \u001b[1;32m~\\.anaconda\\anacon\\Lib\\site-packages\\textblob\\blob.py:371\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, text, tokenizer, pos_tagger, np_extractor, analyzer, parser, classifier, clean_html)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBaseBlob\u001b[39;00m(StringlikeMixin, BlobComparableMixin):\n\u001b[0;32m    353\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"An abstract base class that all textblob classes will inherit from.\u001b[39;00m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;124;03m    Includes words, POS tag, NP, and word count properties. Also includes\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m    basic dunder and string methods for making objects like Python strings.\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \n\u001b[0;32m    357\u001b[0m \u001b[38;5;124;03m    :param text: A string.\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;124;03m    :param tokenizer: (optional) A tokenizer instance. If ``None``,\u001b[39;00m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;124;03m        defaults to :class:`WordTokenizer() <textblob.tokenizers.WordTokenizer>`.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;124;03m    :param np_extractor: (optional) An NPExtractor instance. If ``None``,\u001b[39;00m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m        defaults to :class:`FastNPExtractor() <textblob.en.np_extractors.FastNPExtractor>`.\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;124;03m    :param pos_tagger: (optional) A Tagger instance. If ``None``,\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;124;03m        defaults to :class:`NLTKTagger <textblob.en.taggers.NLTKTagger>`.\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;124;03m    :param analyzer: (optional) A sentiment analyzer. If ``None``,\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;124;03m        defaults to :class:`PatternAnalyzer <textblob.en.sentiments.PatternAnalyzer>`.\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;124;03m    :param parser: A parser. If ``None``, defaults to\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;124;03m        :class:`PatternParser <textblob.en.parsers.PatternParser>`.\u001b[39;00m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;124;03m    :param classifier: A classifier.\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \n\u001b[0;32m    370\u001b[0m \u001b[38;5;124;03m    .. versionchanged:: 0.6.0\u001b[39;00m\n\u001b[1;32m--> 371\u001b[0m \u001b[38;5;124;03m        ``clean_html`` parameter deprecated, as it was in NLTK.\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    373\u001b[0m     np_extractor \u001b[38;5;241m=\u001b[39m FastNPExtractor()\n\u001b[0;32m    374\u001b[0m     pos_tagger \u001b[38;5;241m=\u001b[39m NLTKTagger()\n",
      "\u001b[1;31mTypeError\u001b[0m: The `text` argument passed to `__init__(text)` must be a string, not <class 'list'>"
     ]
    }
   ],
   "source": [
    "[word.lemmatize() for word in TextBlob(cumle).words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "939d2b58-2dec-4cbb-90d5-cb9639767dca",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The `text` argument passed to `__init__(text)` must be a string, not <class 'list'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[218], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m [word\u001b[38;5;241m.\u001b[39mstem() \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m TextBlob(cumle)\u001b[38;5;241m.\u001b[39mwords]\n",
      "File \u001b[1;32m~\\.anaconda\\anacon\\Lib\\site-packages\\textblob\\blob.py:371\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, text, tokenizer, pos_tagger, np_extractor, analyzer, parser, classifier, clean_html)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBaseBlob\u001b[39;00m(StringlikeMixin, BlobComparableMixin):\n\u001b[0;32m    353\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"An abstract base class that all textblob classes will inherit from.\u001b[39;00m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;124;03m    Includes words, POS tag, NP, and word count properties. Also includes\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m    basic dunder and string methods for making objects like Python strings.\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \n\u001b[0;32m    357\u001b[0m \u001b[38;5;124;03m    :param text: A string.\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;124;03m    :param tokenizer: (optional) A tokenizer instance. If ``None``,\u001b[39;00m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;124;03m        defaults to :class:`WordTokenizer() <textblob.tokenizers.WordTokenizer>`.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;124;03m    :param np_extractor: (optional) An NPExtractor instance. If ``None``,\u001b[39;00m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m        defaults to :class:`FastNPExtractor() <textblob.en.np_extractors.FastNPExtractor>`.\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;124;03m    :param pos_tagger: (optional) A Tagger instance. If ``None``,\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;124;03m        defaults to :class:`NLTKTagger <textblob.en.taggers.NLTKTagger>`.\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;124;03m    :param analyzer: (optional) A sentiment analyzer. If ``None``,\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;124;03m        defaults to :class:`PatternAnalyzer <textblob.en.sentiments.PatternAnalyzer>`.\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;124;03m    :param parser: A parser. If ``None``, defaults to\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;124;03m        :class:`PatternParser <textblob.en.parsers.PatternParser>`.\u001b[39;00m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;124;03m    :param classifier: A classifier.\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \n\u001b[0;32m    370\u001b[0m \u001b[38;5;124;03m    .. versionchanged:: 0.6.0\u001b[39;00m\n\u001b[1;32m--> 371\u001b[0m \u001b[38;5;124;03m        ``clean_html`` parameter deprecated, as it was in NLTK.\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    373\u001b[0m     np_extractor \u001b[38;5;241m=\u001b[39m FastNPExtractor()\n\u001b[0;32m    374\u001b[0m     pos_tagger \u001b[38;5;241m=\u001b[39m NLTKTagger()\n",
      "\u001b[1;31mTypeError\u001b[0m: The `text` argument passed to `__init__(text)` must be a string, not <class 'list'>"
     ]
    }
   ],
   "source": [
    "[word.stem() for word in TextBlob(cumle).words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "4591f834-b68f-4a48-ab2b-25a1e7ac0727",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\akbab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "389138de-0d79-42d8-ada9-1c16e480a736",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x:kokbul(x))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df['text']=df['text'].apply(lambda x:kokbul(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1788de62-8766-4a17-81de-1dabbbcd0d96",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df\u001b[38;5;241m.\u001b[39mtext[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aac3ebf-2ccf-48db-847e-bc485d7c976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(lambda x: nfx.remove_stopwords(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "389e1284-dae4-4d55-89c4-a0f004b2047e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ybw\u001b[38;5;241m=\u001b[39mdf[(df\u001b[38;5;241m.\u001b[39mstars\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m|\u001b[39m(df\u001b[38;5;241m.\u001b[39mstars\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m5\u001b[39m)]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "ybw=df[(df.stars==1)|(df.stars==5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201ebc71-d5c5-474b-8ce9-92a6c0764c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "ybw.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e713cabe-88ad-404d-ba87-5e60c1a79541",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=ybw[['text']]\n",
    "y=ybw[['stars']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015ce48b-6b76-47c8-9885-f00f9cdc4d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=trsin_test_split(xyeni,y,random_state=42,test_size=.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5758e9c2-152e-49fd-a0a0-f729a983d052",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CountVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m vect\u001b[38;5;241m=\u001b[39mCountVectorizer(analayzer\u001b[38;5;241m=\u001b[39mkokbul,ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CountVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "vect=CountVectorizer(analayzer=kokbul,ngram_range=(2,3)) #(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18d0f22-5e1d-49fd-a52a-581cfe4fdc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xyeni=vect.fit_transform(['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e339825-a72e-40be-9df8-45972f4c984c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect.get_feature_names_out()[:-50] #[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "221e1444-079d-4137-bbcc-4722e5366623",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trsin_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x_train,x_test,y_train,y_test\u001b[38;5;241m=\u001b[39mtrsin_test_split(xyeni,y,random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.20\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trsin_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test=trsin_test_split(xyeni,y,random_state=42,test_size=.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b547023-933f-461b-93e6-76f85336ecc0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MultibomialNB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m nb\u001b[38;5;241m=\u001b[39mMultibomialNB()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MultibomialNB' is not defined"
     ]
    }
   ],
   "source": [
    "nb=MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8594c0bd-323f-493d-83b8-46d99a05b790",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[38;5;241m=\u001b[39mnb\u001b[38;5;241m.\u001b[39mfit(x_train,y_tarin)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nb' is not defined"
     ]
    }
   ],
   "source": [
    "model=nb.fit(x_train,y_tarin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b806df42-25da-4be4-a21d-57220185ee41",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pred\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(x_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "pred=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eb222c-84d0-4327-862b-09d4efba12ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8e804cb-10f4-4c9d-a59b-e1ed151ccc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Twitter dan veri cekmek ve temizlemek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f2c85e-799f-4266-8e09-2bcf823a09a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ntscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4a5e9b-5022-49f7-8b37-32decaa1cd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ntscraper import Nitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8055151f-87bb-4982-9e5d-231df662ba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts=Nitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a256d7d4-09b4-4f82-a7b2-d666a7e58efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f20ab7f-f1d3-43b4-bf1f-7b18ee0f4d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=ts.get_tweets('Data Science',mode='term',number=50) #tweets=Nitter().get_tweets('Data Science',mode='term',number=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4337b7d-7c7b-4357-b5ff-71a59cd2c1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets=Nitter().get_tweets('Fatihaltayli',mode='user',number=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a00f17-b34b-443a-9a0c-0f3552112993",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(Nitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92aa6a7-3c73-4ea8-bf4b-27c8a931e804",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(tweets['tweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9afca4-0d92-407e-b1af-e863bc1725bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8881c86-9b94-4d5d-ac13-cfaec97eed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['name']=df['user'].apply(lambda x: x['name']) \n",
    "df['name']=df['user'].apply(lambda x: x['username']) \n",
    "df['name']=df['user'].apply(lambda x: x['avatar']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913b7cbb-ce00-4c87-bb9f-2365a8aee00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fce28a4-7756-43a1-a118-018204b86445",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comment']=df['stats'].apply(lambda x: x['comments'])\n",
    "df['likes']=df['stats'].apply(lambda x: x['likes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99549ac1-cf58-4211-ac81-35fd85996492",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a919ef1-b9d3-45ca-ad26-a1eadfa324b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6fb6bd-04d1-4686-8b79-b921036802a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(nfx.remove_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc6b12d-ae33-4502-be76-c1c9f4bbb12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(nfx.remove_userhandles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4325d0-337a-4c12-ae4f-bf439d4bc5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cbe279-1bf6-4767-8353-49d7d46bf21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(nfx.remove_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeb938d-62cc-4133-8225-f494b9f470d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6035b63e-fff7-4fc6-9a98-1ec18dc44a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(nfx.normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d1d5f5-4249-4dd6-8b71-230b6b3b2831",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d49e4bf-bdbb-482d-9b9a-ee94891f604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(nfx.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b59153-0aa3-4e79-b280-c7914e61b9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
